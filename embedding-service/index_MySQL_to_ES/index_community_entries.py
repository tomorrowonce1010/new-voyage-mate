#!/usr/bin/env python
"""index_community_entries.py
å°† MySQL ä¸­çš„ç¤¾åŒºæ¡ç›®æ•°æ®å‘é‡åŒ–å¹¶å†™å…¥ Elasticsearchã€‚
ä¸“é—¨ä¸ºç¤¾åŒºé¡µé¢çš„è¯­ä¹‰æœç´¢åŠŸèƒ½è®¾è®¡ã€‚

è¿è¡Œå‰è¯·å…ˆï¼š
1. pip install -r requirements.txt
2. ç¡®ä¿ Elasticsearch å·²è¿è¡Œä¸”å®‰å…¨å·²å…³é—­æˆ–å·²é…ç½®å¥½è´¦å·å¯†ç 
3. ä¿®æ”¹ä¸‹æ–¹ DB_CONFIG / ES_CONFIG / MODEL_PATH ä¸ºä½ çš„å®é™…ç¯å¢ƒ
4. ç¡®ä¿ MySQL ä¸­æœ‰ç¤¾åŒºæ¡ç›®æ•°æ®ï¼ˆcommunity_entriesè¡¨ï¼‰
"""

import os
from typing import List, Dict
import mysql.connector
from sentence_transformers import SentenceTransformer
from elasticsearch import Elasticsearch, helpers
import time

# -------------------- é…ç½®åŒºåŸŸ --------------------
DB_CONFIG = {
    "host": os.getenv("DB_HOST", "localhost"),
    "port": int(os.getenv("DB_PORT", 3306)),
    "user": os.getenv("DB_USER", "root"),
    "password": os.getenv("DB_PASSWORD", "8001015002yzf"),
    "database": os.getenv("DB_NAME", "voyagemate"),
}

ES_CONFIG = {
    "hosts": [os.getenv("ES_HOST", "http://localhost:9200")],
    "request_timeout": 60,
    "retry_on_timeout": True,
    "verify_certs": False,  # ç¦ç”¨SSLè¯ä¹¦éªŒè¯
    "ssl_show_warn": False,  # ç¦ç”¨SSLè­¦å‘Š
}

INDEX_NAME = os.getenv("ES_INDEX", "community_entries")
MODEL_PATH = os.getenv("MODEL_PATH", "models-chinese")
BATCH_SIZE = 32  # ä¸€æ¬¡ç¼–ç å¤šå°‘æ¡è®°å½•ï¼Œç¤¾åŒºæ¡ç›®é€šå¸¸è¾ƒå°‘ï¼Œä½¿ç”¨è¾ƒå°çš„æ‰¹æ¬¡
# -------------------------------------------------

def fetch_community_entries(conn) -> List[Dict]:
    """ä» MySQL è¯»å–æ‰€æœ‰ç¤¾åŒºæ¡ç›®æ•°æ®ï¼ŒåŒ…å«å…³è”ä¿¡æ¯"""
    cursor = conn.cursor(dictionary=True)
    query = """
    SELECT 
        ce.id,
        ce.share_code,
        ce.description,
        ce.view_count,
        ce.created_at,
        ce.updated_at,
        i.title as itinerary_title,
        i.start_date,
        i.end_date,
        GROUP_CONCAT(DISTINCT d.name SEPARATOR ', ') as destinations,
        u.username as author_username,
        u.id as author_id,
        GROUP_CONCAT(DISTINCT t.tag SEPARATOR ', ') as tags
    FROM community_entries ce
    JOIN itineraries i ON ce.itinerary_id = i.id
    JOIN user u ON i.user_id = u.id
    LEFT JOIN itinerary_days iday ON i.id = iday.itinerary_id
    LEFT JOIN itinerary_activities ia ON iday.id = ia.itinerary_day_id
    LEFT JOIN attractions a ON ia.attraction_id = a.id
    LEFT JOIN destinations d ON a.destination_id = d.id
    LEFT JOIN community_entry_tags cet ON ce.id = cet.share_entry_id
    LEFT JOIN tags t ON cet.tag_id = t.id
    WHERE i.permission_status = 'æ‰€æœ‰äººå¯è§'
    GROUP BY ce.id, ce.share_code, ce.description, ce.view_count, ce.created_at, ce.updated_at,
             i.title, i.start_date, i.end_date, u.username, u.id
    ORDER BY ce.created_at DESC
    """
    cursor.execute(query)
    rows = cursor.fetchall()
    cursor.close()
    return rows

def create_index_if_needed(es: Elasticsearch, dim: int):
    """è‹¥ç´¢å¼•ä¸å­˜åœ¨åˆ™åˆ›å»º"""
    if es.indices.exists(index=INDEX_NAME):
        print(f"Index '{INDEX_NAME}' already exists.")
        return

    mapping = {
        "mappings": {
            "properties": {
                "id": {"type": "long"},
                "share_code": {"type": "keyword"},
                "description": {"type": "text"},
                "view_count": {"type": "integer"},
                "created_at": {"type": "date"},
                "updated_at": {"type": "date"},
                "itinerary_title": {"type": "text"},
                "start_date": {"type": "date"},
                "end_date": {"type": "date"},
                "destinations": {"type": "text"},
                "author_username": {"type": "text"},
                "author_id": {"type": "long"},
                "tags": {"type": "text"},
                "vector": {
                    "type": "dense_vector",
                    "dims": dim,
                    "index": True,
                    "similarity": "cosine",
                },
            }
        }
    }
    es.indices.create(index=INDEX_NAME, body=mapping)
    print(f"Index '{INDEX_NAME}' created.")

def prepare_text_for_embedding(row: Dict) -> str:
    """å‡†å¤‡ç”¨äºå‘é‡åŒ–çš„æ–‡æœ¬"""
    # ç»„åˆæ–‡æœ¬ï¼šè¡Œç¨‹åç§° + æè¿° + ä½œè€…åç§° + ç›®çš„åœ° + æ ‡ç­¾
    text_parts = []
    
    # è¡Œç¨‹åç§°
    if row["itinerary_title"]:
        text_parts.append(row["itinerary_title"])
    
    # æè¿°
    if row["description"]:
        text_parts.append(row["description"])
    
    # ä½œè€…åç§°
    if row["author_username"]:
        text_parts.append(f"ä½œè€…: {row['author_username']}")
    
    # ç›®çš„åœ°
    if row["destinations"]:
        text_parts.append(f"ç›®çš„åœ°: {row['destinations']}")
    
    # æ ‡ç­¾
    if row["tags"]:
        text_parts.append(f"æ ‡ç­¾: {row['tags']}")
    
    return " ".join(text_parts)

def validate_community_entry_data(row: Dict) -> tuple[bool, str]:
    """éªŒè¯ç¤¾åŒºæ¡ç›®æ•°æ®è´¨é‡"""
    # æ£€æŸ¥å¿…è¦å­—æ®µ
    if not row["itinerary_title"] or not row["itinerary_title"].strip():
        return False, "è¡Œç¨‹åç§°ä¸ºç©º"
    
    # æ£€æŸ¥æ–‡æœ¬é•¿åº¦
    text = prepare_text_for_embedding(row)
    if len(text.strip()) < 2:
        return False, f"æ–‡æœ¬å¤ªçŸ­: '{text}'"
    
    # æ£€æŸ¥ç‰¹æ®Šå­—ç¬¦
    title = row["itinerary_title"] or ""
    desc = row["description"] or ""
    
    # æ£€æŸ¥æ§åˆ¶å­—ç¬¦
    control_chars = ['\x00', '\x01', '\x02', '\x03', '\x04', '\x05', '\x06', '\x07', 
                     '\x08', '\x0b', '\x0c', '\x0e', '\x0f']
    
    for char in control_chars:
        if char in title or char in desc:
            return False, f"åŒ…å«æ§åˆ¶å­—ç¬¦: {repr(char)}"
    
    return True, ""

def generate_actions(rows: List[Dict], vecs: List[List[float]]):
    """ç»„è£… bulk actions"""
    for row, vec in zip(rows, vecs):
        try:
            yield {
                "_index": INDEX_NAME,
                "_id": row["id"],
                "_source": {
                    "id": row["id"],
                    "share_code": row["share_code"],
                    "description": row["description"] or "",
                    "view_count": row["view_count"] or 0,
                    "created_at": row["created_at"],
                    "updated_at": row["updated_at"],
                    "itinerary_title": row["itinerary_title"],
                    "start_date": row["start_date"],
                    "end_date": row["end_date"],
                    "destinations": row["destinations"] or "",
                    "author_username": row["author_username"],
                    "author_id": row["author_id"],
                    "tags": row["tags"] or "",
                    "vector": vec,
                },
            }
        except Exception as e:
            print(f"ç”Ÿæˆactionæ—¶å‡ºé”™ (ID {row['id']}): {e}")
            continue

def main():
    print(f"=== å¼€å§‹ä¸ºç¤¾åŒºæ¡ç›®åˆ›å»ºå‘é‡ç´¢å¼• ===")
    
    # 1. åˆå§‹åŒ–èµ„æº
    print("1. åˆå§‹åŒ–æ¨¡å‹å’Œè¿æ¥...")
    try:
        model = SentenceTransformer(MODEL_PATH, device="cpu")
        es = Elasticsearch(**ES_CONFIG)
        
        if not es.ping():
            raise RuntimeError(f"Elasticsearch cluster is not reachable")
        
        conn = mysql.connector.connect(**DB_CONFIG)
        dim = model.get_sentence_embedding_dimension()
        print(f"Model loaded. Embedding dim = {dim}")
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        return

    # 2. åˆ›å»ºç´¢å¼•
    try:
        create_index_if_needed(es, dim)
    except Exception as e:
        print(f"âŒ åˆ›å»ºç´¢å¼•å¤±è´¥: {e}")
        return

    # 3. è¯»å–æ•°æ®
    print("2. è¯»å–ç¤¾åŒºæ¡ç›®æ•°æ®...")
    try:
        rows = fetch_community_entries(conn)
        print(f"Fetched {len(rows)} community entry records.")
    except Exception as e:
        print(f"âŒ è¯»å–æ•°æ®å¤±è´¥: {e}")
        return

    if len(rows) == 0:
        print("è­¦å‘Šï¼šæ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç¤¾åŒºæ¡ç›®æ•°æ®ï¼")
        return

    # 4. æ•°æ®è´¨é‡æ£€æŸ¥
    print("3. æ•°æ®è´¨é‡æ£€æŸ¥...")
    valid_rows = []
    invalid_rows = []
    
    for row in rows:
        is_valid, reason = validate_community_entry_data(row)
        if is_valid:
            valid_rows.append(row)
        else:
            invalid_rows.append({
                "id": row["id"],
                "title": row["itinerary_title"],
                "reason": reason
            })
    
    print(f"æœ‰æ•ˆæ•°æ®: {len(valid_rows)} ä¸ªæ¡ç›®")
    print(f"æ— æ•ˆæ•°æ®: {len(invalid_rows)} ä¸ªæ¡ç›®")
    
    if invalid_rows:
        print("æ— æ•ˆæ¡ç›®è¯¦æƒ…:")
        for invalid in invalid_rows[:10]:
            print(f"  - ID {invalid['id']}: {invalid['title']} - {invalid['reason']}")
        if len(invalid_rows) > 10:
            print(f"  ... è¿˜æœ‰ {len(invalid_rows) - 10} ä¸ªæ— æ•ˆæ¡ç›®")

    # 5. æ‰¹é‡å‘é‡åŒ– + å†™å…¥ ES
    print("4. å¼€å§‹å‘é‡åŒ–å’Œç´¢å¼•...")
    total_indexed = 0
    failed_indices = []
    start_time = time.time()
    
    for i in range(0, len(valid_rows), BATCH_SIZE):
        batch = valid_rows[i : i + BATCH_SIZE]
        batch_num = i // BATCH_SIZE + 1
        total_batches = (len(valid_rows) + BATCH_SIZE - 1) // BATCH_SIZE
        
        try:
            print(f"å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch)} ä¸ªæ¡ç›®)...")
            
            # å‡†å¤‡æ–‡æœ¬
            texts = []
            for row in batch:
                try:
                    text = prepare_text_for_embedding(row)
                    texts.append(text)
                except Exception as e:
                    failed_indices.append({
                        "id": row["id"],
                        "title": row["itinerary_title"],
                        "error": f"æ–‡æœ¬å‡†å¤‡å¤±è´¥: {e}"
                    })
                    print(f"å‡†å¤‡æ–‡æœ¬å¤±è´¥ (ID {row['id']}): {e}")
                    continue
            
            if not texts:
                print(f"æ‰¹æ¬¡ {batch_num} æ²¡æœ‰æœ‰æ•ˆæ–‡æœ¬ï¼Œè·³è¿‡")
                continue
            
            # å‘é‡åŒ–
            try:
                vecs = model.encode(texts, normalize_embeddings=True).tolist()
            except Exception as e:
                print(f"å‘é‡åŒ–å¤±è´¥ (æ‰¹æ¬¡ {batch_num}): {e}")
                for row in batch:
                    failed_indices.append({
                        "id": row["id"],
                        "title": row["itinerary_title"],
                        "error": f"å‘é‡åŒ–å¤±è´¥: {e}"
                    })
                continue
            
            # æ‰¹é‡å†™å…¥ES
            try:
                actions = list(generate_actions(batch, vecs))
                if actions:
                    success_count, errors = helpers.bulk(es, actions, raise_on_error=False)
                    total_indexed += success_count
                    
                    if errors:
                        for error in errors:
                            failed_indices.append({
                                "id": error.get("index", {}).get("_id", "unknown"),
                                "title": "unknown",
                                "error": f"ESå†™å…¥å¤±è´¥: {error.get('index', {}).get('error', {}).get('reason', 'unknown error')}"
                            })
                    
                    print(f"æ‰¹æ¬¡ {batch_num} å®Œæˆ: {success_count}/{len(batch)} æˆåŠŸ")
                else:
                    print(f"æ‰¹æ¬¡ {batch_num} æ²¡æœ‰æœ‰æ•ˆactions")
                    
            except Exception as e:
                print(f"ESæ‰¹é‡å†™å…¥å¤±è´¥ (æ‰¹æ¬¡ {batch_num}): {e}")
                for row in batch:
                    failed_indices.append({
                        "id": row["id"],
                        "title": row["itinerary_title"],
                        "error": f"ESå†™å…¥å¤±è´¥: {e}"
                    })
            
        except Exception as e:
            print(f"å¤„ç†æ‰¹æ¬¡ {batch_num} æ—¶å‡ºé”™: {e}")
            for row in batch:
                failed_indices.append({
                    "id": row["id"],
                    "title": row["itinerary_title"],
                    "error": f"æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}"
                })

    # 6. éªŒè¯ç´¢å¼•ç»“æœ
    processing_time = time.time() - start_time
    print("5. éªŒè¯ç´¢å¼•ç»“æœ...")
    try:
        stats = es.indices.stats(index=INDEX_NAME)
        actual_count = stats['indices'][INDEX_NAME]['total']['docs']['count']
        print(f"ç´¢å¼•éªŒè¯ï¼šElasticsearchä¸­å®é™…ç´¢å¼•äº† {actual_count} ä¸ªæ¡ç›®")
        
        expected_count = len(valid_rows)
        if actual_count != expected_count:
            print(f"âš ï¸  è­¦å‘Šï¼šç´¢å¼•æ•°é‡ä¸åŒ¹é…ã€‚æœ‰æ•ˆæ•°æ®{expected_count}ä¸ªï¼Œä½†Elasticsearchä¸­åªæœ‰{actual_count}ä¸ª")
            if failed_indices:
                print(f"å¤±è´¥ç´¢å¼•çš„æ¡ç›®æ•°é‡: {len(failed_indices)}")
                print("å‰10ä¸ªå¤±è´¥çš„æ¡ç›®:")
                for i, failed in enumerate(failed_indices[:10]):
                    print(f"  {i+1}. ID {failed['id']}: {failed['title']} - {failed['error']}")
                if len(failed_indices) > 10:
                    print(f"  ... è¿˜æœ‰ {len(failed_indices) - 10} ä¸ªå¤±è´¥çš„æ¡ç›®")
        else:
            print("âœ… ç´¢å¼•æ•°é‡åŒ¹é…ï¼")
            
    except Exception as e:
        print(f"éªŒè¯ç´¢å¼•ç»“æœæ—¶å‡ºé”™: {e}")

    # 7. æ€»ç»“
    print("=== ç´¢å¼•å®Œæˆ ===")
    print(f"ç´¢å¼•åç§°ï¼š{INDEX_NAME}")
    print(f"åŸå§‹æ¡ç›®æ•°é‡ï¼š{len(rows)}")
    print(f"æœ‰æ•ˆæ¡ç›®æ•°é‡ï¼š{len(valid_rows)}")
    print(f"æ— æ•ˆæ¡ç›®æ•°é‡ï¼š{len(invalid_rows)}")
    print(f"æˆåŠŸç´¢å¼•æ•°é‡ï¼š{total_indexed}")
    print(f"å¤±è´¥ç´¢å¼•æ•°é‡ï¼š{len(failed_indices)}")
    print(f"å‘é‡ç»´åº¦ï¼š{dim}")
    print(f"å¤„ç†è€—æ—¶ï¼š{processing_time:.2f}ç§’")
    
    if invalid_rows or failed_indices:
        print("\nğŸ’¡ å»ºè®®:")
        print("1. æ£€æŸ¥å¤±è´¥çš„æ¡ç›®æ•°æ®è´¨é‡")
        print("2. æ¸…ç†æˆ–ä¿®å¤æ— æ•ˆçš„æ¡ç›®æ•°æ®")
        print("3. é‡æ–°è¿è¡Œç´¢å¼•è„šæœ¬")
    
    conn.close()

if __name__ == "__main__":
    main()

# -------------------- requirements.txt å»ºè®® --------------------
# sentence-transformers==2.6.1
# torch>=2.2.0
# mysql-connector-python==8.3.0
# elasticsearch==8.13.0
# tqdm==4.66.2 