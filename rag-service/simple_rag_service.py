#!/usr/bin/env python3
"""
ç®€åŒ–çš„RAGæœåŠ¡
æä¾›åŸå¸‚æ—…æ¸¸é—®ç­”åŠŸèƒ½
"""
import os
import json
from typing import List, Tuple
import faiss
from fastapi import FastAPI, Query, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from sentence_transformers import SentenceTransformer
from src.llm import answer_with_deepseek

# é…ç½® - ä¿®æ”¹ä¸ºä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„
MODEL_BASE_PATH = os.path.dirname(os.path.abspath(__file__))
EMBED_MODEL = os.path.join(MODEL_BASE_PATH, "models", "paraphrase-multilingual-MiniLM-L12-v2")
INDEX_DIR = "index"
DATA_DIR = "data/processed"

app = FastAPI(title="ç®€åŒ–RAGæ™ºèƒ½ä¼´æ¸¸åŠ©æ‰‹")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class SimpleRAGService:
    def __init__(self):
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        self._check_model_files()
        
        # åŠ è½½æœ¬åœ°æ¨¡å‹
        print(f"æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹: {EMBED_MODEL}")
        self.embedder = SentenceTransformer(EMBED_MODEL)
        
        self.index = None
        self.chunks = []
        self.metas = []
        self._load_knowledge_base()
    
    def _check_model_files(self):
        """æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§"""
        required_files = [
            "config.json",
            "model.safetensors",  # æˆ–è€… pytorch_model.bin
            "tokenizer_config.json",
            "vocab.txt",
            "sentence_bert_config.json"
        ]
        
        missing_files = []
        for file in required_files:
            file_path = os.path.join(EMBED_MODEL, file)
            if not os.path.exists(file_path):
                missing_files.append(file)
        
        if missing_files:
            print(f"âš ï¸ ç¼ºå°‘ä»¥ä¸‹æ¨¡å‹æ–‡ä»¶: {missing_files}")
            print("å°è¯•ç»§ç»­åŠ è½½ï¼Œä½†å¯èƒ½ä¼šé‡åˆ°é—®é¢˜...")
        else:
            print("âœ… æ‰€æœ‰å¿…éœ€çš„æ¨¡å‹æ–‡ä»¶éƒ½å­˜åœ¨")
        
        # åˆ—å‡ºå®é™…å­˜åœ¨çš„æ–‡ä»¶
        existing_files = os.listdir(EMBED_MODEL)
        print(f"æ¨¡å‹ç›®å½•ä¸­çš„æ–‡ä»¶: {existing_files}")
    
    def _load_knowledge_base(self):
        """åŠ è½½çŸ¥è¯†åº“"""
        try:
            # æ£€æŸ¥ç´¢å¼•ç›®å½•æ˜¯å¦å­˜åœ¨
            if not os.path.exists(INDEX_DIR):
                raise FileNotFoundError(f"ç´¢å¼•ç›®å½•ä¸å­˜åœ¨: {INDEX_DIR}")
            
            # æŸ¥æ‰¾å¯ç”¨çš„ç´¢å¼•æ–‡ä»¶
            index_files = [f for f in os.listdir(INDEX_DIR) if f.endswith('.faiss')]
            if not index_files:
                raise FileNotFoundError("æœªæ‰¾åˆ°FAISSç´¢å¼•æ–‡ä»¶")
            
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„ç´¢å¼•
            index_name = index_files[0].replace('.faiss', '')
            index_path = os.path.join(INDEX_DIR, f"{index_name}.faiss")
            meta_path = os.path.join(INDEX_DIR, f"{index_name}_meta.json")
            chunks_path = os.path.join(DATA_DIR, f"{index_name}.jsonl")
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            for path in [index_path, meta_path, chunks_path]:
                if not os.path.exists(path):
                    raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {path}")
            
            # åŠ è½½ç´¢å¼•
            self.index = faiss.read_index(index_path)
            
            # åŠ è½½å…ƒæ•°æ®
            with open(meta_path, 'r', encoding='utf-8') as f:
                meta_data = json.load(f)
                self.metas = meta_data['metas']
            
            # åŠ è½½æ–‡æœ¬å—
            with open(chunks_path, 'r', encoding='utf-8') as f:
                self.chunks = [json.loads(line) for line in f]
            
            print(f"âœ… çŸ¥è¯†åº“åŠ è½½æˆåŠŸ: {len(self.chunks)} ä¸ªæ–‡æœ¬å—")
            
        except Exception as e:
            print(f"âŒ çŸ¥è¯†åº“åŠ è½½å¤±è´¥: {e}")
            # åˆ›å»ºä¸€ä¸ªç©ºçš„ç´¢å¼•å’Œæ–‡æœ¬å—ï¼Œè®©æœåŠ¡è‡³å°‘èƒ½å¯åŠ¨
            self.index = None
            self.chunks = []
            self.metas = []
    
    def search(self, query: str, top_k: int = 5) -> str:
        """æœç´¢ç›¸å…³æ–‡æœ¬"""
        if self.index is None:
            # å¦‚æœçŸ¥è¯†åº“æœªåŠ è½½ï¼Œè¿”å›ç©ºä¸Šä¸‹æ–‡
            return "çŸ¥è¯†åº“æš‚ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ç´¢å¼•æ–‡ä»¶ã€‚"
        
        try:
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_embedding = self.embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True)
            
            # æœç´¢
            scores, indices = self.index.search(query_embedding, top_k)
            
            # æ„å»ºä¸Šä¸‹æ–‡
            context_parts = []
            for i, idx in enumerate(indices[0]):
                if idx < len(self.chunks):
                    chunk = self.chunks[idx]
                    source = chunk.get('source', 'æœªçŸ¥æ¥æº')
                    text = chunk.get('text', '')
                    score = scores[0][i]
                    context_parts.append(f"æ¥æº: {source} (ç›¸ä¼¼åº¦: {score:.3f})\n\n{text}")
            
            if not context_parts:
                return "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
            
            return "\n\n---\n\n".join(context_parts)
            
        except Exception as e:
            return f"æœç´¢è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}"

# å…¨å±€RAGæœåŠ¡å®ä¾‹
rag_service = None

@app.on_event("startup")
async def startup_event():
    """å¯åŠ¨æ—¶åŠ è½½çŸ¥è¯†åº“"""
    global rag_service
    try:
        rag_service = SimpleRAGService()
        print("ğŸš€ RAGæœåŠ¡å¯åŠ¨æˆåŠŸ")
    except Exception as e:
        print(f"âŒ RAGæœåŠ¡å¯åŠ¨å¤±è´¥: {e}")

@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "ç®€åŒ–RAGæ™ºèƒ½ä¼´æ¸¸åŠ©æ‰‹",
        "status": "running",
        "model_path": EMBED_MODEL,
        "endpoints": {
            "/health": "å¥åº·æ£€æŸ¥",
            "/search": "æœç´¢ç›¸å…³æ–‡æœ¬",
            "/ask": "æ™ºèƒ½é—®ç­”"
        }
    }

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    status = "healthy"
    if rag_service is None or rag_service.index is None:
        status = "degraded"
    
    return {
        "status": status,
        "knowledge_base_loaded": rag_service is not None and rag_service.index is not None,
        "chunks_count": len(rag_service.chunks) if rag_service else 0,
        "model_loaded": rag_service is not None and rag_service.embedder is not None
    }

@app.get("/search")
async def search_text(query: str = Query(..., min_length=2), top_k: int = 5):
    """æœç´¢ç›¸å…³æ–‡æœ¬"""
    if rag_service is None:
        raise HTTPException(status_code=500, detail="RAGæœåŠ¡æœªåˆå§‹åŒ–")
    
    try:
        context = rag_service.search(query, top_k)
        return {
            "query": query,
            "context": context,
            "chunks_found": len(context.split("---")) if context and "---" in context else 0
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æœç´¢å¤±è´¥: {str(e)}")

@app.get("/ask")
async def ask_question(question: str = Query(..., min_length=2), top_k: int = 5):
    """æ™ºèƒ½é—®ç­”"""
    if rag_service is None:
        raise HTTPException(status_code=500, detail="RAGæœåŠ¡æœªåˆå§‹åŒ–")
    
    try:
        # æœç´¢ç›¸å…³æ–‡æœ¬
        context = rag_service.search(question, top_k)
        
        # ç”Ÿæˆå›ç­”
        answer = answer_with_deepseek(question, context)
        
        return {
            "question": question,
            "answer": answer,
            "context": context,
            "chunks_used": len(context.split("---")) if context and "---" in context else 0
        }
    except Exception as e:
        return {
            "question": question,
            "answer": f"æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆå›ç­”: {str(e)}",
            "context": rag_service.search(question, top_k) if rag_service else "",
            "error": str(e)
        }

if __name__ == "__main__":
    import uvicorn
    import os
    port = int(os.getenv("PORT", "8001"))
    uvicorn.run(app, host="0.0.0.0", port=port)
